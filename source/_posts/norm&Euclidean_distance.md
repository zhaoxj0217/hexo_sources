---

title: 机器学习中的范数及各种距离—数学基础 
data: 2017-11-16
categories: mathematics,Machine Learning
tags: [mathematics,Machine Learning,norm]

---

转自http://blog.csdn.net/shijing_0214/article/details/51757564  

**1.什么是范数**  
&emsp;&emsp;范数(norm)是数学中的一种基本概念。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。  

&emsp;&emsp;有时候为了便于理解，我们可以把范数当作距离来理解。   

&emsp;&emsp;在数学上，范数包括向量范数和矩阵范数，**向量范数**表征向量空间中向量的大小，**矩阵范数**表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算AX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。

**1、 L-P范数**    
&emsp;&emsp;在机器学习中一般使用L-P向量范数，L-P范数不是一个范数，而是一组范数，其定义如下：  
![norm_pic1.png](https://i.loli.net/2017/11/16/5a0d3dcb5161b.png)

根据P 的变化，范数也有着不同的变化，一个经典的有关P范数的变化图如下：   
![norm_pic2.png](https://i.loli.net/2017/11/16/5a0d40fe578e6.png)

上图表示了p从无穷到0变化时，三维空间中到原点的距离（范数）为1的点构成的图形的变化情况。以常见的L-2范数（p=2）为例，此时的范数也即欧氏距离，空间中到原点的欧氏距离为1的点构成了一个球面。  


实际上，在0≤p<1时，Lp并不满足三角不等式的性质，也就不是严格意义下的范数。以p=0.5，二维坐标(1,4)、(4,1)、(1,9)为例，不满足三角不等式。因此这里的L-P范数只是一个概念上的宽泛说法。

闵（明）可夫斯基距离(Minkowski Distance)，闵氏距离（明式）不是一种距离，而是一组距离的定义，对应Lp范数 
等于1时候，其公式等价于曼哈顿距离。
等于2时候，其公式等价于欧式距离。
当大于2到无穷大时候，其公式等价于切比雪夫距离。


**2、L0范数**  
&emsp;&emsp;当P=0时，也就是L0范数，由上面可知，L0范数并不是一个真正的范数，它主要被用来度量向量中非零元素的个数。用上面的L-P定义可以得到的L-0的定义为：   
![norm_pic3.PNG](https://i.loli.net/2017/11/16/5a0d44d13d6cb.png)

这里就有点问题了，我们知道非零元素的零次方为1，但零的零次方，非零数开零次方都是什么鬼，很不好说明L0的意义，所以在通常情况下，大家都用的是： ||x||0=#(i|xi≠0)   
表示向量x中非零元素的个数。
对于L0范数，其优化问题为：   
![norm_pic4.PNG](https://i.loli.net/2017/11/16/5a0d451e0d809.png)
  
在实际应用中，由于L0范数本身不容易有一个好的数学表示形式，给出上面问题的形式化表示是一个很难的问题，故被人认为是一个NP难问题。所以在实际情况中，L0的最优问题会被放宽到L1或L2下的最优化。

**3、L1范数**   
&emsp;&emsp;L1范数是我们经常见到的一种范数，它的定义如下：
&emsp;&emsp;||x||1=∑i|xi|  
&emsp;&emsp;表示向量x中非零元素的绝对值之和。  
&emsp;&emsp;L1范数有很多的名字，例如我们熟悉的曼哈顿距离、最小绝对误差等。使用L1范数可以度量两个向量间的差异，如绝对误差和（Sum of Absolute Difference）： 
SAD(x1,x2)=∑i|x1i−x2i|    
&emsp;&emsp;对于L1范数，它的优化问题如下：   
![norm_pic5.PNG](https://i.loli.net/2017/11/16/5a0d45d48c9b4.png)  
&emsp;&emsp;由于L1范数的天然性质，对L1优化的解是一个稀疏解，因此L1范数也被叫做稀疏规则算子。通过L1可以实现特征的稀疏，去掉一些没有信息的特征，例如在对用户的电影爱好做分类的时候，用户有100个特征，可能只有十几个特征是对分类有用的，大部分特征如身高体重等可能都是无用的，利用L1范数就可以过滤掉。

**4、L2范数**   
&emsp;&emsp;L2范数是我们最常见最常用的范数了，我们用的最多的度量距离欧氏距离就是一种L2范数，它的定义如下   
![norm_pic6.PNG](https://i.loli.net/2017/11/16/5a0d482636904.png)

&emsp;&emsp;表示向量元素的平方和再开平方。 
&emsp;&emsp;像L1范数一样，L2也可以度量两个向量间的差异，如平方差和（Sum of Squared Difference）: SSD(x1,x2)=∑i(x1i−x2i)2

&emsp;&emsp;对于L2范数，它的优化问题如下：   
![norm_pic7.PNG](https://i.loli.net/2017/11/16/5a0d4860a15c2.png)

&emsp;&emsp;L2范数通常会被用来做优化目标函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。

欧式距离的向量运算的形式：  
![norm_pic10.PNG](http://img.my.csdn.net/uploads/201211/20/1353399664_2255.png)


**5、L-∞范数**   
&emsp;&emsp;当P=∞时，也就是L-∞范数，它主要被用来度量向量元素的最大值。用上面的L-P定义可以得到的L∞的定义为：  
![norm_pic8.PNG](https://i.loli.net/2017/11/16/5a0d48b452e16.png)   

&emsp;&emsp;与L0一样，在通常情况下，大家都用的是：||x||∞=max(|xi|) 来表示L∞   
&emsp;&emsp;无穷范数对应切比雪夫距离：若二个向量或二个点x1和x2，其坐标分别为(x11, x12, x13, ... , x1n)和(x21, x22, x23, ... , x2n)，则二者的切比雪夫距离为：d = max(|x1i - x2i|)，i从1到n

&emsp;&emsp;上句摘自维基百科，但是这玩意鬼看得懂啊，为了更好的理解切比雪夫距离，在这里举一个通俗易懂的例子：   
&emsp;&emsp;比如，有同样两个人，在纽约准备到北京参拜天安门，同一个地点出发的话，按照欧式距离来计算，是完全一样的。   
&emsp;&emsp;但是按照切比雪夫距离，这是完全不同的概念了。      
&emsp;&emsp;譬如，其中一个人是土豪，另一个人是中产阶级，第一个人就能当晚直接头等舱走人，而第二个人可能就要等机票什么时候打折再去，或者选择坐船什么的。    
&emsp;&emsp;这样来看的话，距离是不是就不一样了呢？    
&emsp;&emsp;或者还是不清楚，再说的详细点。   
&emsp;&emsp;同样是这两个人，欧式距离是直接算最短距离的，而切比雪夫距离可能还得加上财力，比如第一个人财富值100，第二个只有30，虽然物理距离一样，但是所包含的内容却是不同的。

**其他：汉明距离**   
&emsp;&emsp;汉明距离是使用在数据传输差错控制编码里面的，汉明距离是一个概念，它表示两个（相同长度）字对应位不同的数量，我们以d（x,y）表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。

![norm_pic9.PNG](https://i.loli.net/2017/11/16/5a0d496f50d6e.png)