---

title: 决策树之ID3算法
data: 2017-11-29
categories: decision tree,Machine Learning,ID3
tags: [decision tree,Machine Learning]

---

信息增益（香农熵/熵）  
熵是表示随机变量不确定性的度量  
条件熵H（Y|X）表示在已知随机变量X的条件下随机变量Y的不确定性
信息增益表示得知特征X的信息而使得类Y的信息不确定性减少的程度
定义:特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H（D）与特征A给定条件下D的经验条件熵H（D|A）之差，及g(D,A)=H(D)-H(D|A)  
信息增益比：特征A对训练数据集D的信息增益比gr(D,A)定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比。 

ID3算法生成决策树  
ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。具体的方法是：从根节点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的额特征作为结点的特征，由该特征的不同取值简历子节点；再对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。直到得到一个决策树。

ID3算法  
基于奥卡姆剃刀原理的,即用尽量用较少的东西做更多的事  
是一种自顶向下的贪心策略  
输入：训练数据集D，特征集A，阈值ε  
输出：决策树T  
（1）若D中所有实例属于同一类Ck,则T为单结点树，并将类Ck作该结点的类标记，返回T；
（2）若A=Ø，则T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T；
（3）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征Ag；
（4）如果Ag的信息增益小于阈值ε，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T；
（5）否则，对Ag的每一个可能值ai，依Ag=ai将D分割为若干非空子集Di,将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T
（6）对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归地调用步1~步5，得到子树Ti,返回Ti


贷款申请样本数据表

| ID | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |   
| 1  | 青年 | 否     | 否         | 一般     | 否 |  
| 2  | 青年 | 否     | 否         | 好       | 否 |  
| 3  | 青年 | 是     | 否         | 好       | 是 |  
| 4  | 青年 | 是     | 是         | 一般     | 是 |  
| 5  | 青年 | 否     | 否         | 一般     | 否 |  
| 6  | 中年 | 否     | 否         | 一般     | 否 |  
| 7  | 中年 | 否     | 否         | 好       | 否 |  
| 8  | 中年 | 是     | 是         | 好       | 是 |  
| 9  | 中年 | 否     | 是         | 非常好   | 是 |  
| 10 | 中年 | 否     | 是         | 非常好   | 是 |  
| 11 | 老年 | 否     | 是         | 非常好   | 是 |  
| 12 | 老年 | 否     | 是         | 好       | 是 |  
| 13 | 老年 | 是     | 否         | 好       | 是 |  
| 14 | 老年 | 是     | 否         | 非常好    | 是 |  
| 15 | 老年 | 否     | 否         | 一般      | 否 |  

首先计算经验熵H（D）  
![HD.PNG](https://i.loli.net/2017/12/04/5a251868957b2.png)  
再分别以A1,A2,A3,A4表示年龄，有工作，有自己的房子和信贷情况4个特征，计算信息增益  
年龄  
![gDA1.PNG](https://i.loli.net/2017/12/04/5a251918f40e6.png)   

有工作   
![gDA2.PNG](https://i.loli.net/2017/12/04/5a25198a32c9c.png)

有自己的房子  
![gDA3.PNG](https://i.loli.net/2017/12/04/5a251af3d52bb.png)  

信贷情况 略
g(D|A4) = 0.971-0.608 =0.363  

最后，由于特征A3的信息增益最大，所以选择特征A3作为最优特征
以A3为根结点的特征，将训练数据集D划分为两个子集D1（A3取值为是）和D2（A3取值为否）由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为是
对D2从A1年龄，A2有工作，和A4信贷情况中选择新的特征

![gD2A.PNG](https://i.loli.net/2017/12/04/5a251f3e86411.png)  


选择信息增益最大的特征A2作为节点的特征，由于A2有两个可能取值，从这一节点引出两个子节点，一个对应“是”，包含3个样本，它们属于同一类，所以这是一个叶结点，另一个是对应“否”，包含6个样本，他们也属于同一类，所以这也是一个叶结点，类标记为“否”
最终生成决策树  


![decision_final.PNG](https://i.loli.net/2017/12/04/5a2522a745f9a.png)















